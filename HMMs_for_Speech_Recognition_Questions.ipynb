{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HMMs for Speech Recognition Questions.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ94jA0SuXpy",
        "colab_type": "text"
      },
      "source": [
        "# Lecture - What are Hidden Markov Models (HMMs)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ScGwB2MEwxdV"
      },
      "source": [
        "\n",
        "Hidden Markov Models is a type of generative modelling technique, where the underlying process is assumed to be a Markov process with hidden states. The key property of the Markov process is that each event only depends on the previous state.\n",
        "\n",
        "## **Why are we discussing them?**\n",
        "\n",
        "At the time of writing these notes, HMM-DNN models are still the state of the art techniques for speech recognition. They have been superseded by WaveNet for speech synthesis only recently. This means we should spend a little time in order to understand these systems, or potentially develop them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdqtQi3PxTvi",
        "colab_type": "text"
      },
      "source": [
        "## Mathematical scope\n",
        "\n",
        "We are going to discuss Markov chains that are discrete in time and state too. Mathematical detail is kept as brief as it is possible.\n",
        "\n",
        "A Markov chain consists of a transition matrix $ A_{ij} $, an initial probability distribution $ \\pi $. These are sufficient to describe an evolution of Markov chain. \n",
        "\n",
        "Consider a 2-state Markov chain for example. It could be described by,\n",
        "\n",
        "$$ A = \\begin{bmatrix}0.2 & 0.5\\\\0.8 & 0.5\\end{bmatrix} \\quad \\pi = \\begin{bmatrix} 0.1 \\\\ 0.9 \\end{bmatrix} $$ \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVpuqIQj0eZm",
        "colab_type": "code",
        "outputId": "299d2fc9-b61f-4505-962c-25c7e1374876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "A = np.array([[0.2,0.5],[0.8,0.5]])\n",
        "start = np.array([[0.1],[0.9]])\n",
        "state = start\n",
        "\n",
        "T = 6\n",
        "for t in range(1,T):\n",
        "  state = np.dot(A,state)\n",
        "  plt.subplot(int(T/2),2,t)\n",
        "  plt.bar(np.array([0,1]),state[:,0],color=[\"r\",\"b\"])\n",
        "  \n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBxhQJp3pluS",
        "colab_type": "text"
      },
      "source": [
        "## Stationary distributions\n",
        "\n",
        "Under certrain conditions for a Markov chain, its stationary distribution can be calculated. The stationary distribution is the distribution the Markov chain \"tends to\" after some amount of time. \n",
        "\n",
        "But how can we calculate this? We have seen that $ \\mathbf{A} \\mathbf{v}_{t} = \\mathbf{v}_{t+1} $ is the rule to calculate one step in the evolution. We are looking for the case, where\n",
        "\n",
        "$$ \\mathbf{A} \\mathbf{v}_{t} = \\mathbf{v}_{t}. $$\n",
        "\n",
        "You might remember seeing this equation in a different form somewhere else, like this\n",
        "\n",
        "$$ \\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}, $$\n",
        "\n",
        "which is called the eigenvalue equation. Indeed the stationary distribution is the eigenvector of the transition matrix corresponding to an eigenvalue of $ \\lambda = 1 $.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRJOOOBH5qiU",
        "colab_type": "text"
      },
      "source": [
        "## Graphical representation\n",
        "\n",
        "Markov chains are often represented using this kind of graphical representation. The nodes represent the states and the edges represent the transition probabilities.\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Finance_Markov_chain_example_state_space.svg/400px-Finance_Markov_chain_example_state_space.svg.png)\n",
        "\n",
        "\n",
        "Q: Can you construct the transition probablity matrix based on this diagram?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9Fr96hz7Etl",
        "colab_type": "text"
      },
      "source": [
        "## Hidden Markov models\n",
        "\n",
        "The only difference between Markov chains and hidden Markov models now is that the **states of the HMM are not directly observed**, instead the underlying states emit the phenomenon that we observe. \n",
        "\n",
        "![HMM example](https://miro.medium.com/max/1200/1*0xjHjL19uK0d6llcEJ0Z0w.png)\n",
        "\n",
        "In the graph above, we can see that there are two states: **rainy** and **sunny**. These are statements about a weather on any given day. As previously, the normal edges indicate the transition probabilities. For example, given a rainy day, there is a 0.7 probability it will be a rainy day again.\n",
        "\n",
        "The dashed lines indicate activities that we might do depending the weather. These are the activities that we observe: walking, shopping and cleaning. Similarly to the state transition, the events that happen based on these states are not deterministic. These events probabilities are called **emission probabilities**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHsmtWDf-S_C",
        "colab_type": "text"
      },
      "source": [
        "## Basic inference mechanics\n",
        "\n",
        "We are not going to be able to cover Forward-Backward, Baum-Welch and Viterbi algorithm for HMM inference. \n",
        "\n",
        "\n",
        "*   There are extensive derivations for these algorithms in the literature, I will give reference too.\n",
        "*   The reward for the intellectual challenge of understanding these algorithms is however not great enough. It is very rare, that someone does not use an out-of-box inference solution for HMMs.\n",
        "* What makes sense however is to look at a few examples of simpler inference, and point out where difficulties arise.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB9wb7YYAUb7",
        "colab_type": "text"
      },
      "source": [
        "### Find out probability of an observation\n",
        "\n",
        "$$ P(X_2 = \\text{shopping}) = ? $$\n",
        "\n",
        "We know that this could have happened two ways\n",
        "\n",
        "$$ P(X_2 = \\text{sh}) = P(X_2 = \\text{sh} | Z_2 = \\text{r} ) + P(X_2 = \\text{sh} | Z_2 = \\text{s} ) $$\n",
        "\n",
        "Now, we can use the Markovian mechanics for modelling the transitions. We will assume an initial distribution $ P(Z_1) = [0.5,0.5] $ \n",
        "\n",
        "\n",
        "To avoid lengthy calculations, we can just evolve the transition matrix one-step ahead first by,\n",
        "\n",
        "$$ A = \\begin{bmatrix}0.7 & 0.4\\\\0.3 & 0.6\\end{bmatrix}  \\begin{bmatrix}0.5 \\\\ 0.5\\end{bmatrix} = \\begin{bmatrix}0.55 \\\\ 0.45\\end{bmatrix} = \\begin{bmatrix}P(Z_2=r) \\\\ P(Z_2=s)\\end{bmatrix} $$ \n",
        "\n",
        "Now, we can calculate the emission probabilities,\n",
        "\n",
        "$$ P(X_2 = \\text{sh}) = P(X_2 = \\text{sh} | Z_2 = \\text{r} ) + P(X_2 = \\text{sh} | Z_2 = \\text{s} ) = 0.4 \\cdot 0.55 + 0.3 \\cdot 0.45 = 0.355 $$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDqoz5O3j3yv",
        "colab_type": "text"
      },
      "source": [
        "## But how can I predict my words using this setup?\n",
        "\n",
        "Using generatives models such as HMM requires a shift in our understanding. What we most often do, i.e with a linear classifier is the following:\n",
        "\n",
        "$$ w^Tx = y \\quad \\begin{cases} 1 \\quad y \\geq 0.5 \\\\ 0 \\quad y < 0.5  \\end{cases} $$\n",
        "\n",
        "For multiclass problems, however this is not sufficient. As you may know, what a neural network often does is to output a probability distribution at the last layer, i.e. for digit classification it would look like:\n",
        "\n",
        "$$ [0.1, 0.1, 0.1, 0.5, 0.05, 0.05, 0.05,0.05, 0, 0] $$\n",
        "\n",
        "which would mean that we should classify our example as a 3 in this case.\n",
        "\n",
        "A generative model is more similar to that. Following the inference mechanics above, we can train **one model for each word**. \n",
        "\n",
        "The results of each generative model can be then incorporated into a vector like the one above.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0Tb7esLnxIk",
        "colab_type": "text"
      },
      "source": [
        "## Duration modelling\n",
        "\n",
        "Our previous example of HMM is not one that is actually used in speech recognition. All models used for speech recognition are so called left-to-right HMMs. \n",
        "\n",
        "\n",
        "![From David Weenink's lecture notes](https://i.imgur.com/eAKmcIt.png)\n",
        "\n",
        "\n",
        "\n",
        "Q: Can you figure out why they are called left-to-right HMMs?\n",
        "\n",
        "Apart from the left-to-right directions however, self-transitions are also allowed. This is useful for modelling durations as we expect that the calculated features that we are observing do not change between succesive time-intervals that much. (**quasi-stationary assumption**)\n",
        "\n",
        "For example, a larger self-transition probability means a longer duration.\n",
        "\n",
        "Notice, how these properties are reflected in the matrix domain for a 3-state left-to-right HMM:\n",
        "\n",
        "$$ A = \\begin{bmatrix}0.7 & 0.3 & 0.0 \\\\ 0 & 0.4 & 0.6 \\\\ 0 & 0 & 1 \\end{bmatrix}   $$ \n",
        "\n",
        "Because only right movements are allowed, and only one step at a time, each row will have at most two elements. Also, the last state will be a so called **absorbing/absorption state**, once our \"HMM particle\" arrives there it cannot escape from there. Another property to notice is that this matrix is going to be upper triangular always. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKs5SPFTrizX",
        "colab_type": "text"
      },
      "source": [
        "## Emission probabilities as Gaussian clusters\n",
        "\n",
        "Instead of fixed emission probabilities, it is more realistic to use probability distributions (Gaussians or Gaussian mixtures usually due to tractability issues). The qualitative reason for that is simply to account for variations in speech -- not everyone pronounced the same word the same way. From a modelling perspective, this allows greater flexibility of the model, as the transition matrix is quite constrained on itself.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK5fTdEVwguu",
        "colab_type": "text"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp6p4HfOsEta",
        "colab_type": "text"
      },
      "source": [
        "**Question 1.** Consider a left-to-right HMM with the transition matrix above. Can you say anything about its stationary distribution? Why do you think this property is useful for modelling speech?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_AoXJdLsk1Y",
        "colab_type": "text"
      },
      "source": [
        "**Question 2.** Word/vowel recognition. Using the module hmmlearn, you are going to build your first speech recognition module. You can use your own PC or this Google Colaboratory for doing the exercise.\n",
        "\n",
        "You can either use the vowel dataset with the formants alreadz processed. \n",
        "\n",
        "A word dataset could be downloaded here:\n",
        "https://code.google.com/archive/p/hmm-speech-recognition/downloads\n",
        "\n",
        "The documentation of the hmmlearn module can be found here: https://hmmlearn.readthedocs.io/en/latest/api.html#gaussianhmm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCP8hVN2tMCl",
        "colab_type": "text"
      },
      "source": [
        "**Part A. Data loading** Load your data from your Google Drive using the code snippet below. Make sure that your data is accessible in convenient numpy format. For the vowel CSV (which is actually a \"TSV\"), use pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr8TRbe0HjnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.listdir(\"drive/My Drive/audio\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6iOYTkavMmG",
        "colab_type": "text"
      },
      "source": [
        "**Part B. Data preprocessing** We are going to use Mel-frequency cepstral coefficients for data preprocessing from the module librosa. Note that (quite annoyingly) there are subtle differences between various MFCC implementations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGcjHdtRYfd7",
        "colab_type": "text"
      },
      "source": [
        "**Part C.** Train-test partitioning. Use stratified partioning, held-out validation will be good enough for our purposes and to limit computations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Ap7FbivxZi",
        "colab_type": "text"
      },
      "source": [
        "**Part 3. HMM modelling** Train a HMM for each of your vowels/words with a few components. Note, that we want a left-to-right HMM so you have to initialise your transition matrix to be upper triangular and also your initial distribution to start at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Bu04E2HWK8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install hmmlearn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQjKGXQDwTxq",
        "colab_type": "text"
      },
      "source": [
        "**Part 4. Evaluation.** Check model performance on a validation/test set. To evaluate the mistakes (if there are any) create a confusion matrix. If you have time, try to reason why do you think the misclassifications happened and how would you change the model accordingly? (Apart from using deep learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWj-bYASw12p",
        "colab_type": "text"
      },
      "source": [
        "# Further references\n",
        "\n",
        "I hope you have enjoyed this assignment and learned a lot. Note, that this has been only an introduction to speech recognition. It is a very intersting and active field, but it can be overwhelming to find where to start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-YwjDQ1xM3j",
        "colab_type": "text"
      },
      "source": [
        "Most research in automatic speech recognition happens using the **Kaldi** framework. The framework is overwhelming to understand for beginners and takes a lot of time to install. \n",
        "\n",
        "*   **Kaldi installation guide** http://jrmeyer.github.io/asr/2016/01/26/Installing-Kaldi.html \n",
        "*   **Official Kaldi site** https://kaldi-asr.org/\n",
        "\n",
        "There are other options, which are reasonably popular:\n",
        "\n",
        "*   HTK - http://htk.eng.cam.ac.uk/\n",
        "*   CMUSphinx - https://cmusphinx.github.io/\n",
        "\n"
      ]
    }
  ]
}